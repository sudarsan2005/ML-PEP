{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TEXT PROCESSING**\n",
        "\n",
        "\n",
        "1.  Segmentation\n",
        "2.  Tokenization\n",
        "3. Lemmatization\n",
        "4.  Stemming\n",
        "5.  Stop Words Removing\n",
        "6.  Noun Entity Relationship\n",
        "\n"
      ],
      "metadata": {
        "id": "vCHESTSiQnQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries for NLP**\n",
        "\n",
        "\n",
        "1.   NLTK\n",
        "2.   SpaCy\n",
        "3.   Gensim\n",
        "4. GTTS\n",
        "5.Pyttsx3\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jTgqZt54LX9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Segmentation\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "text=\"Hi welcome to this workshop. Here we are going to see how NLP works\"\n",
        "sentences=sent_tokenize(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyx3qHC9Led8",
        "outputId": "ab7d2c7e-52f3-4ae4-eb33-f9676a14685c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(\"                  \",sentence)\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypWr6DfgMXux",
        "outputId": "1821051c-910d-44a5-ecae-41691337f15a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Hi welcome to this workshop.\n",
            "\n",
            "\n",
            "\n",
            "                   Here we are going to see how NLP works\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "words=word_tokenize(text)\n",
        "for word in words:\n",
        "  print(\"         \",word)\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnSssvZ4NUGc",
        "outputId": "710bd31b-3adb-4d10-a969-98aaccf6c5b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Hi\n",
            "\n",
            "\n",
            "          welcome\n",
            "\n",
            "\n",
            "          to\n",
            "\n",
            "\n",
            "          this\n",
            "\n",
            "\n",
            "          workshop\n",
            "\n",
            "\n",
            "          .\n",
            "\n",
            "\n",
            "          Here\n",
            "\n",
            "\n",
            "          we\n",
            "\n",
            "\n",
            "          are\n",
            "\n",
            "\n",
            "          going\n",
            "\n",
            "\n",
            "          to\n",
            "\n",
            "\n",
            "          see\n",
            "\n",
            "\n",
            "          how\n",
            "\n",
            "\n",
            "          NLP\n",
            "\n",
            "\n",
            "          works\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "words=[\"Connection\",\"Connected\",\"Connecting\"]\n",
        "for word in words:\n",
        "  print(\"         \",ps.stem(word))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS5VpefbNt4N",
        "outputId": "37bcb14b-796e-46da-99ab-070eeed04d2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          connect\n",
            "\n",
            "\n",
            "          connect\n",
            "\n",
            "\n",
            "          connect\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "words=[\"Universal\",\"Universe\",\"University\"]\n",
        "for word in words:\n",
        "  print(\"         \",ps.stem(word))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COlk52_mO-lt",
        "outputId": "55e45951-4c48-4cd3-a655-a6a44f2cd599"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          univers\n",
            "\n",
            "\n",
            "          univers\n",
            "\n",
            "\n",
            "          univers\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "words=[\"alumnus\",\"alumnae\",\"alumni\"]\n",
        "for word in words:\n",
        "  print(\"         \",ps.stem(word))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw59HuHFPyb3",
        "outputId": "e6941642-11d4-4fce-c7fb-d3e536bc9497"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          alumnu\n",
            "\n",
            "\n",
            "          alumna\n",
            "\n",
            "\n",
            "          alumni\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "ss=PorterStemmer()\n",
        "words=[\"alumnus\",\"alumnae\",\"alumni\"]\n",
        "for word in words:\n",
        "  print(\"         \",ss.stem(word))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz4J2KKsQC81",
        "outputId": "91751859-ff07-458e-a800-30c78bd4c39a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          alumnu\n",
            "\n",
            "\n",
            "          alumna\n",
            "\n",
            "\n",
            "          alumni\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "ls=LancasterStemmer()\n",
        "words=[\"alumnus\",\"alumnae\",\"alumni\"]\n",
        "for word in words:\n",
        "  print(\"         \",ls.stem(word))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBfCrCCkQ9lo",
        "outputId": "fc41cfbd-c731-47ab-d907-2119108bdcf1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          alumn\n",
            "\n",
            "\n",
            "          alumna\n",
            "\n",
            "\n",
            "          alumn\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Lemmatizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYa4Qt94RHUp",
        "outputId": "b9f55bbc-c245-468c-b152-553fd1cb4390"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Using Noun Tag\")\n",
        "print(lemmatizer.lemmatize('is'))\n",
        "print(lemmatizer.lemmatize('are'))\n",
        "print(lemmatizer.lemmatize('was'))\n",
        "print(lemmatizer.lemmatize('were'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGebgTa0RTvK",
        "outputId": "757f3d5b-e36b-42f6-ad98-b50608589f59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Noun Tag\n",
            "is\n",
            "are\n",
            "wa\n",
            "were\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Using Verb \")\n",
        "print(lemmatizer.lemmatize('working',pos='v'))\n",
        "print(lemmatizer.lemmatize('worked',pos='v'))\n",
        "print(lemmatizer.lemmatize('was',pos='v'))\n",
        "print(lemmatizer.lemmatize('were',pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmDwGBhaRkUr",
        "outputId": "41065d17-d9c1-4411-a4b6-6d9b51ddf230"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Verb \n",
            "work\n",
            "work\n",
            "be\n",
            "be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)\n",
        "text=text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlcTJzQFSeh5",
        "outputId": "6114c5ca-d8b9-47bb-b326-84cf49d3a28a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi welcome to this workshop. Here we are going to see how NLP works\n",
            "hi welcome to this workshop. here we are going to see how nlp works\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Stop Words Removal\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words=stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YZeHbN1S_ie",
        "outputId": "e0c9e9d2-25b2-44ed-a5f6-da8f3d0df789"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_text=[word for word in nltk.word_tokenize(text) if word.lower() not in stop_words]\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIY8VKHKTYMK",
        "outputId": "ef97d79d-a85a-4fe1-d14f-657927b7e1cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'welcome', 'workshop', '.', 'going', 'see', 'nlp', 'works']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: POS Tagging\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.chunk import ne_chunk_sents\n",
        "tokens=nltk.word_tokenize(text)\n",
        "pos_tags=pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqIkWB0fUYV2",
        "outputId": "0cfdb087-600c-4f56-f126-f0af8759955a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hi', 'NN'), ('welcome', 'NN'), ('to', 'TO'), ('this', 'DT'), ('workshop', 'NN'), ('.', '.'), ('here', 'RB'), ('we', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('see', 'VB'), ('how', 'WRB'), ('nlp', 'JJ'), ('works', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Sample documents and their corresponding labels\n",
        "documents = [\n",
        "    \"This is a positive review.\",\n",
        "    \"This is a negative review.\",\n",
        "    \"Another positive review.\",\n",
        "    \"A very negative review.\"\n",
        "]\n",
        "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Creating a CountVectorizer object to convert text to BoW representation\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fitting the vectorizer to the documents and transform them\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "print(X)\n",
        "\n",
        "# Creating a Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Training the classifier\n",
        "classifier.fit(X, labels)\n",
        "\n",
        "#sample text\n",
        "new_document = input()\n",
        "\n",
        "# Convert the sample text to BoW representation\n",
        "X_new = vectorizer.transform([new_document])\n",
        "\n",
        "# Predict the label for the new document\n",
        "predicted_label = classifier.predict(X_new)[0]\n",
        "\n",
        "# Print the predicted label\n",
        "print(\"Predicted label:\", predicted_label)\n",
        "\n",
        "\n",
        "# It may fail due to the lack of data used for training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gxMz7bcWz7s",
        "outputId": "27f7b025-6c54-4a2e-b1e4-4d5826d67b25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 1 1 0]\n",
            " [0 1 1 0 1 1 0]\n",
            " [1 0 0 1 1 0 0]\n",
            " [0 0 1 0 1 0 1]]\n",
            "stay positive\n",
            "Predicted label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Sample documents and their corresponding labels\n",
        "documents = [\n",
        "    \"This is a positive review.Stay Positive\",\n",
        "    \"This is a negative review.\",\n",
        "    \"Another positive review.\",\n",
        "    \"A very negative review.\"\n",
        "]\n",
        "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Creating a TF-IDF Vectorizer object to convert text to BoW representation\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fitting the vectorizer to the documents and transform them\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "print(X)\n",
        "\n",
        "# Creating a Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Training the classifier\n",
        "classifier.fit(X, labels)\n",
        "\n",
        "#sample text\n",
        "new_document = input()\n",
        "\n",
        "# Convert the sample text to BoW representation\n",
        "X_new = vectorizer.transform([new_document])\n",
        "\n",
        "# Predict the label for the new document\n",
        "predicted_label = classifier.predict(X_new)[0]\n",
        "\n",
        "# Print the predicted label\n",
        "print(\"Predicted label:\", predicted_label)\n",
        "\n",
        "\n",
        "# It may fail due to the lack of data used for training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN3yE5NwlXiN",
        "outputId": "ad0dcba4-869d-464d-e050-0e9dca36844d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.35252226 0.         0.70504452 0.23333087 0.44712979\n",
            "  0.35252226 0.        ]\n",
            " [0.         0.53931298 0.53931298 0.         0.35696573 0.\n",
            "  0.53931298 0.        ]\n",
            " [0.72664149 0.         0.         0.5728925  0.37919167 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.5728925  0.         0.37919167 0.\n",
            "  0.         0.72664149]]\n",
            "stay negative\n",
            "Predicted label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPm0QYQil0n3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}